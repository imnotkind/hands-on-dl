{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # only required to use numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            textbook matrix shape (row x column)\n",
    "                x : 2 x ?\n",
    "                w : 2 x 1\n",
    "                wTx : 1 x ?\n",
    "                y : 1 x ?\n",
    "\n",
    "            this code matrix shape (row x column)\n",
    "                x : ? x 2\n",
    "                w : 2 x 1\n",
    "                xw : ? x 1\n",
    "                y : ? x 1\n",
    "        '''\n",
    "        self.w = np.random.rand(2,1) # weight initialization\n",
    "        self.b = 0 #np.random.rand(1) # bias initialization\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "            Activation function of logistic regression\n",
    "            input :\n",
    "                z : ? x 1\n",
    "            output :\n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        '''\n",
    "            Predict using logistic regression\n",
    "            input : \n",
    "                data(x) : ? x 2\n",
    "                weight(w) : 2 x 1\n",
    "                bias(b) : 1 x 1 => ? x 1 (broadcast)\n",
    "            output : \n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(self.sigmoid(x @ self.w + self.b))\n",
    "        else:\n",
    "            return self.sigmoid(x @ self.w + self.b) # numpy broadcast b\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        '''\n",
    "            Cost function of logistic regression\n",
    "            input : \n",
    "                prediction(y_hat) : ? x 1\n",
    "                label(y) : ? x 1\n",
    "            output : \n",
    "                cost function value : 1 x 1\n",
    "        '''\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = len(y)\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        y_hat = self.predict(x)\n",
    "        \n",
    "        m = len(x)\n",
    "        \n",
    "        dz = y_hat - y # dz : ? x 1\n",
    "        dw = (1 / m) * (x.T @  dz)# x : 2 x ?\n",
    "        db = (1 / m) * np.sum(dz)\n",
    "        \n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            prev_w = self.w\n",
    "            prev_b = self.b\n",
    "            prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "            \n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if prev_cost < self.cost(self.predict(val_x, classify=False), val_y): \n",
    "                self.w = prev_w\n",
    "                self.b = prev_b\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                print(\"Validation Break!\")\n",
    "                break\n",
    "            \n",
    "            if i % 50000 == 0:\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHiddenLayer:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            textbook matrix shape (row x column)\n",
    "                x : 2 x ?\n",
    "                w : 2 x 1\n",
    "                wTx : 1 x ?\n",
    "                y : 1 x ?\n",
    "\n",
    "            this code matrix shape (row x column)\n",
    "                x : ? x 2\n",
    "                w : 2 x 1\n",
    "                xw : ? x 1\n",
    "                y : ? x 1\n",
    "        '''\n",
    "        self.w = np.random.rand(2,1) # weight initialization\n",
    "        self.b = 0 #np.random.rand(1) # bias initialization\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "            Activation function of logistic regression\n",
    "            input :\n",
    "                z : ? x 1\n",
    "            output :\n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        '''\n",
    "            Predict using logistic regression\n",
    "            input : \n",
    "                data(x) : ? x 2\n",
    "                weight(w) : 2 x 1\n",
    "                bias(b) : 1 x 1 => ? x 1 (broadcast)\n",
    "            output : \n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(self.sigmoid(x @ self.w + self.b))\n",
    "        else:\n",
    "            return self.sigmoid(x @ self.w + self.b) # numpy broadcast b\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        '''\n",
    "            Cost function of logistic regression\n",
    "            input : \n",
    "                prediction(y_hat) : ? x 1\n",
    "                label(y) : ? x 1\n",
    "            output : \n",
    "                cost function value : 1 x 1\n",
    "        '''\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = len(y)\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        y_hat = self.predict(x)\n",
    "        \n",
    "        m = len(x)\n",
    "        \n",
    "        dz = y_hat - y # dz : ? x 1\n",
    "        dw = (1 / m) * (x.T @  dz)# x : 2 x ?\n",
    "        db = (1 / m) * np.sum(dz)\n",
    "        \n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            prev_w = self.w\n",
    "            prev_b = self.b\n",
    "            prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "            \n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if prev_cost < self.cost(self.predict(val_x, classify=False), val_y): \n",
    "                self.w = prev_w\n",
    "                self.b = prev_b\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                print(\"Validation Break!\")\n",
    "                break\n",
    "            \n",
    "            if i % 50000 == 0:\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "\n",
    "noises = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "data = [{\"train\" : None, \"val\": None, \"test\": None} for _ in range(len(noises))]\n",
    "\n",
    "for i in range(len(noises)):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_file = open(\"two_moon_{}/train.txt\".format(noises[i]), \"r\")\n",
    "    for line in train_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        train_x.append([float(l[0]), float(l[1])])\n",
    "        train_y.append([int(l[2])])\n",
    "    train_file.close()\n",
    "    data[i][\"train\"] = (np.array(train_x), np.array(train_y))\n",
    "\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    val_file = open(\"two_moon_{}/val.txt\".format(noises[i]), \"r\")\n",
    "    for line in val_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        val_x.append([float(l[0]), float(l[1])])\n",
    "        val_y.append([int(l[2])])\n",
    "    val_file.close()\n",
    "    data[i][\"val\"] = (np.array(val_x), np.array(val_y))\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_file = open(\"two_moon_{}/test.txt\".format(noises[i]), \"r\")\n",
    "    for line in test_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        test_x.append([float(l[0]), float(l[1])])\n",
    "        test_y.append([int(l[2])])\n",
    "    test_file.close()\n",
    "    data[i][\"test\"] = (np.array(test_x), np.array(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Cost: 0.5985651868598194\n",
      "Iteration: 50000 Cost: 0.3276535230614428\n",
      "Iteration: 100000 Cost: 0.309041311070134\n",
      "Iteration: 150000 Cost: 0.30628119114996954\n",
      "Iteration: 153092 Cost: 0.3062756414817539\n",
      "Validation Break!\n",
      "Noise: 0.1 Accuracy: 0.87\n",
      "Iteration: 0 Cost: 0.7447162934146244\n",
      "Iteration: 50000 Cost: 0.34103718496095625\n",
      "Iteration: 100000 Cost: 0.32713537530991305\n",
      "Iteration: 150000 Cost: 0.3241935800959835\n",
      "Iteration: 200000 Cost: 0.32328624214805557\n",
      "Iteration: 250000 Cost: 0.3229430866021468\n",
      "Iteration: 300000 Cost: 0.32279487917986993\n",
      "Iteration: 350000 Cost: 0.32272509059166404\n",
      "Iteration: 400000 Cost: 0.3226904358097971\n",
      "Iteration: 450000 Cost: 0.32267268743259325\n",
      "Noise: 0.3 Accuracy: 0.85\n",
      "Iteration: 0 Cost: 0.8164066388651605\n",
      "Iteration: 50000 Cost: 0.45597878641415907\n",
      "Iteration: 100000 Cost: 0.45393427576564804\n",
      "Iteration: 150000 Cost: 0.45377109355553047\n",
      "Iteration: 200000 Cost: 0.4537501431744659\n",
      "Iteration: 250000 Cost: 0.4537471044598486\n",
      "Iteration: 300000 Cost: 0.45374665201774356\n",
      "Iteration: 350000 Cost: 0.45374658428154163\n",
      "Iteration: 400000 Cost: 0.4537465741286357\n",
      "Iteration: 450000 Cost: 0.45374657260643203\n",
      "Noise: 0.5 Accuracy: 0.85\n",
      "Iteration: 0 Cost: 0.6955605970749901\n",
      "Iteration: 16486 Cost: 0.5085185275462722\n",
      "Validation Break!\n",
      "Noise: 0.7 Accuracy: 0.76\n",
      "Iteration: 0 Cost: 0.7298933775885433\n",
      "Iteration: 31991 Cost: 0.5350712037002449\n",
      "Validation Break!\n",
      "Noise: 0.9 Accuracy: 0.745\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(noises)):\n",
    "    train_x, train_y = data[i][\"train\"]\n",
    "    val_x, val_y = data[i][\"val\"]\n",
    "    test_x, test_y = data[i][\"test\"]\n",
    "    \n",
    "    prob_1 = LogisticRegression()\n",
    "    prob_1.train(train_x, train_y, val_x, val_y, lr=0.001, iters=200000)\n",
    "    print(\"Noise:\", noises[i], \"Accuracy:\", prob_1.accuracy(test_x, test_y))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
