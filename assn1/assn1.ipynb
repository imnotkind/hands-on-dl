{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # only required to use numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            textbook matrix shape (row x column)\n",
    "                x : 2 x 100\n",
    "                w : 2 x 1\n",
    "                wTx : 1 x 100\n",
    "                y : 1 x 100\n",
    "\n",
    "            this code matrix shape (row x column)\n",
    "                x : 100 x 2\n",
    "                w : 2 x 1\n",
    "                xw : 100 x 1\n",
    "                y : 100 x 1\n",
    "        '''\n",
    "        self.w = np.random.rand(2,1) # weight initialization\n",
    "        self.b = np.random.rand(1) # bias initialization\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "            Activation function of logistic regression\n",
    "            input :\n",
    "                z : ? x 1\n",
    "            output :\n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        '''\n",
    "            Predict using logistic regression\n",
    "            input : \n",
    "                data(x) : ? x 2\n",
    "                weight(w) : 2 x 1\n",
    "                bias(b) : 1 x 1 => ? x 1 (broadcast)\n",
    "            output : \n",
    "                prediction(y_hat) : ? x 1\n",
    "        '''\n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(self.sigmoid(x @ self.w + self.b))\n",
    "        else:\n",
    "            return self.sigmoid(x @ self.w + self.b) # numpy broadcast b\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        '''\n",
    "            Cost function of logistic regression\n",
    "            input : \n",
    "                prediction(y_hat) : ? x 1\n",
    "                label(y) : ? x 1\n",
    "            output : \n",
    "                cost function value : 1 x 1\n",
    "        '''\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = len(y)\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        y_hat = self.predict(x)\n",
    "        \n",
    "        m = len(x)\n",
    "        \n",
    "        dz = y_hat - y # dz : ? x 1\n",
    "        dw = (1 / m) * (x.T @  dz)# x : 2 x ?\n",
    "        db = (1 / m) * np.sum(dz)\n",
    "        \n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            prev_w = self.w\n",
    "            prev_b = self.b\n",
    "            prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "            \n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if prev_cost < self.cost(self.predict(val_x, classify=False), val_y): \n",
    "                self.w = prev_w\n",
    "                self.b = prev_b\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                print(\"Validation Break!\")\n",
    "                break\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                print(\"Iteration:\", i ,\"Cost:\", prev_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHiddenLayer:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "\n",
    "train_x = []\n",
    "train_y = []\n",
    "train_file = open(\"two_moon_0.2/train.txt\", \"r\")\n",
    "for line in train_file.readlines():\n",
    "    l = line.rstrip().split()\n",
    "    train_x.append([float(l[0]), float(l[1])])\n",
    "    train_y.append([int(l[2])])\n",
    "train_file.close()\n",
    "train_x = np.array(train_x)\n",
    "train_y = np.array(train_y)\n",
    "\n",
    "val_x = []\n",
    "val_y = []\n",
    "val_file = open(\"two_moon_0.2/val.txt\", \"r\")\n",
    "for line in val_file.readlines():\n",
    "    l = line.rstrip().split()\n",
    "    val_x.append([float(l[0]), float(l[1])])\n",
    "    val_y.append([int(l[2])])\n",
    "val_file.close()\n",
    "val_x = np.array(val_x)\n",
    "val_y = np.array(val_y)\n",
    "\n",
    "test_x = []\n",
    "test_y = []\n",
    "test_file = open(\"two_moon_0.2/test.txt\", \"r\")\n",
    "for line in test_file.readlines():\n",
    "    l = line.rstrip().split()\n",
    "    test_x.append([float(l[0]), float(l[1])])\n",
    "    test_y.append([int(l[2])])\n",
    "test_file.close()\n",
    "test_x = np.array(test_x)\n",
    "test_y = np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0 Cost: 0.664228042132798\n",
      "Iteration: 10000 Cost: 0.390747601863068\n",
      "Iteration: 20000 Cost: 0.35114008628589843\n",
      "Iteration: 30000 Cost: 0.33446026638651577\n",
      "Iteration: 40000 Cost: 0.32547245739628433\n",
      "Iteration: 50000 Cost: 0.3201359487257466\n",
      "Iteration: 60000 Cost: 0.31683810479649566\n",
      "Iteration: 70000 Cost: 0.3147798648777434\n",
      "Iteration: 80000 Cost: 0.3135125218486045\n",
      "Iteration: 90000 Cost: 0.3127646333865339\n",
      "Iteration: 100000 Cost: 0.3123640964089413\n",
      "Iteration: 110000 Cost: 0.31219823333719904\n",
      "Iteration: 115359 Cost: 0.3121785676340696\n",
      "Validation Break!\n",
      "Full iteration complete\n",
      "Accuracy: 0.845\n"
     ]
    }
   ],
   "source": [
    "prob_1 = LogisticRegression()\n",
    "prob_1.train(train_x, train_y, val_x, val_y, lr=0.001, iters=300000)\n",
    "print(\"Accuracy:\", prob_1.accuracy(test_x, test_y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
