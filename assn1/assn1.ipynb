{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "## Dataset\n",
    "two moon dataset with noise 0.1, 0.3, 0.5, 0.7, 0.9\n",
    "- x : 2 x ?  \n",
    "- y : 1 x ?\n",
    "\n",
    "## Logistic Regression\n",
    "| Logistic Regression | 0.1  | 0.3  | 0.5  | 0.7   | 0.9   |\n",
    "|---------------------|------|------|------|-------|-------|\n",
    "| accuracy            | 0.84 | 0.85 | 0.85 | 0.755 | 0.785 |\n",
    "\n",
    "You can see that accuracy generally decreases as noise gets bigger, which is quite natural, because noise represents inconsistent, unseperable, faulty data.\n",
    "\n",
    "## One Hidden Layer NN\n",
    "| One Hidden Layer | 0.1   | 0.3   | 0.5   | 0.7   | 0.9   |\n",
    "|------------------|-------|-------|-------|-------|-------|\n",
    "| 1                | 0.84  | 0.85  | 0.855 | 0.76  | 0.78  |\n",
    "| 3                | 1.0   | 0.895 | 0.84  | 0.755 | 0.795 |\n",
    "| 5                | 0.84  | 0.84  | 0.865 | 0.765 | 0.785 |\n",
    "| 7                | 0.845 | 0.895 | 0.865 | 0.725 | 0.78  |\n",
    "| 9                | 0.845 | 0.895 | 0.865 | 0.73  | 0.078 |\n",
    "| Optimal          | 3     | 3     | 5     | 5     | 3     |\n",
    "\n",
    "I chose the optimal layer with the standard of\n",
    "- having the highest accuracy in that dataset\n",
    "- if accuracy score is the same, choose the one that has less hidden units (simpler)\n",
    "\n",
    "You can see that accuracy generally decreases as noise gets bigger.  \n",
    "\n",
    "You can also see that 3~5 hidden units have the best accuracy, and even if you have more(or less) hidden units than that, it doesn't help you in raising accuracy, rather it lowers the accuracy.\n",
    "\n",
    "## Two Hidden Layer NN\n",
    "| Two Hidden Layer | 0.1  | 0.3   | 0.5   | 0.7   | 0.9   |\n",
    "|------------------|------|-------|-------|-------|-------|\n",
    "| 1x1              | 0.84 | 0.84  | 0.855 | 0.76  | 0.775 |\n",
    "| 1x3              | 0.84 | 0.84  | 0.855 | 0.76  | 0.775 |\n",
    "| 1x5              | 0.84 | 0.84  | 0.855 | 0.765 | 0.775 |\n",
    "| 1x7              | 0.84 | 0.84  | 0.855 | 0.76  | 0.775 |\n",
    "| 1x9              | 0.84 | 0.84  | 0.855 | 0.76  | 0.775 |\n",
    "| 3x1              | 1.0  | 0.895 | 0.865 | 0.735 | 0.775 |\n",
    "| 3x3              | 1.0  | 0.835 | 0.855 | 0.735 | 0.775 |\n",
    "| 3x5              | 1.0  | 0.895 | 0.86  | 0.765 | 0.78  |\n",
    "| 3x7              | 1.0  | 0.9   | 0.86  | 0.755 | 0.775 |\n",
    "| 3x9              | 1.0  | 0.895 | 0.86  | 0.765 | 0.775 |\n",
    "| 5x1              | 1.0  | 0.835 | 0.85  | 0.735 | 0.78  |\n",
    "| 5x3              | 1.0  | 0.895 | 0.86  | 0.755 | 0.775 |\n",
    "| 5x5              | 1.0  | 0.895 | 0.865 | 0.725 | 0.775 |\n",
    "| 5x7              | 1.0  | 0.84  | 0.855 | 0.755 | 0.775 |\n",
    "| 5x9              | 1.0  | 0.895 | 0.86  | 0.76  | 0.785 |\n",
    "| 7x1              | 1.0  | 0.895 | 0.86  | 0.735 | 0.465 |\n",
    "| 7x3              | 1.0  | 0.89  | 0.855 | 0.735 | 0.785 |\n",
    "| 7x5              | 1.0  | 0.89  | 0.87  | 0.735 | 0.785 |\n",
    "| 7x7              | 1.0  | 0.89  | 0.86  | 0.735 | 0.78  |\n",
    "| 7x9              | 1.0  | 0.895 | 0.86  | 0.735 | 0.78  |\n",
    "| 9x1              | 1.0  | 0.49  | 0.86  | 0.535 | 0.455 |\n",
    "| 9x3              | 1.0  | 0.895 | 0.865 | 0.715 | 0.785 |\n",
    "| 9x5              | 1.0  | 0.895 | 0.845 | 0.755 | 0.78  |\n",
    "| 9x7              | 1.0  | 0.89  | 0.85  | 0.735 | 0.78  |\n",
    "| 9x9              | 1.0  | 0.895 | 0.87  | 0.755 | 0.795 |\n",
    "| Optimal          | 3x1  | 3x7   | 7x5   | 1x5   | 9x9   |\n",
    "\n",
    "I chose the optimal layer with the standard of\n",
    "- having the highest accuracy in that dataset\n",
    "- if accuracy score is the same, choose the one that has less hidden units (simpler)\n",
    "\n",
    "You can see that accuracy generally decreases as noise gets bigger.  \n",
    "\n",
    "When there are less noise, having more layers can generally improve accuracy.  \n",
    "When there are too much noise, having more layers won't help that much, because the train data doesn't represent the test data so much.  \n",
    "\n",
    "The data has more relation to the number of hidden layers in the first layer, and less relation to the second layer, as you can see clearly in noise 0.1.\n",
    "\n",
    "## Overall\n",
    "\n",
    "\n",
    "I stopped training when the cost of validation set starts to increase while training, to prevent overfitting.  \n",
    "Maybe this lead to a learning failure in some complicated networks, since I didn't give them enough time for the cost function to constantly decrease in the early iterations. You could see some accuracy scores stopping on near 0.5, which is caused by a break in the loop for gaining worse scores on validation set. To prevent this, I think I should give more iterations until I evaluate the validation score in complex models.\n",
    "\n",
    "You could see that there are times when validation cost is higher, but test accuracy is also higher.  \n",
    "This could happen because we use a different validation set and test set.  \n",
    "\n",
    "My gradient calculation is all based on the course pdf. (Which helped A LOT thank you)  \n",
    "The accuracy calculation is based on how many inferences the model got it correct in the test dataset, which the model hasn't seen in training.\n",
    "\n",
    "When there is too much noise, the train data doesn't represent the true data, which means complicated classifiers won't have much of a meaning, since it would only try to learn more of the train data.  \n",
    "Even when there is less noise, if you see the chart of one hidden layer noise 0.1, complex models(5,7,9) did not beat 3 hidden units, which means they tried to overfit before they learned the general tendency of the data, leading to a validation score loss and thus a termination on learning.\n",
    "\n",
    "If you see the two hidden layer case, you could see some correlation in the number of first layers. I think this is due to gradient vanishing, so the first layer gets less different than the second layer, resulting to similar output with the same number of first layers.\n",
    "\n",
    "I learned that complicated networks are hard to learn, and it is not necessarily better in accuracy too.  \n",
    "I think the optimal network is decided by how the data looks like, and I think the two moon dataset can be naturally classified in only 3 hidden units, so more than that could cause only overfitting.\n",
    "\n",
    "Full output can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # only required to use numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "            matrix shape (row x column)\n",
    "                x : 2 x ?\n",
    "                w : 2 x 1\n",
    "                b : 1 x 1\n",
    "                wTx : 1 x ?\n",
    "                y : 1 x ?\n",
    "        '''\n",
    "        self.w = np.random.rand(2,1) # weight initialization\n",
    "        self.b = 0 # bias initialization\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        '''\n",
    "            Activation function of logistic regression\n",
    "            input :\n",
    "                z : 1 x ?\n",
    "            output :\n",
    "                prediction(y_hat) : 1 x ?\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        '''\n",
    "            Predict using logistic regression\n",
    "            input : \n",
    "                data(x) : 2 x ?\n",
    "            output : \n",
    "                prediction(y_hat) : 1 x ?\n",
    "        '''\n",
    "        z = self.w.T @ x + self.b # z : 1 x ?\n",
    "        a = self.sigmoid(z) # a : 1 x ?\n",
    "        \n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(a)\n",
    "        else:\n",
    "            return a\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        '''\n",
    "            Cost function of logistic regression\n",
    "            input : \n",
    "                prediction(y_hat) : 1 x ?\n",
    "                label(y) : 1 x ?\n",
    "            output : \n",
    "                cost function value : 1 x 1\n",
    "        '''\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = y.shape[1]\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        y_hat = self.predict(x)\n",
    "        \n",
    "        m = y.shape[1]\n",
    "        \n",
    "        dz = y_hat - y # dz : 1 x ?\n",
    "        dw = (1 / m) * (x @ dz.T) # x : 2 x ?, w : 2 x 1\n",
    "        db = (1 / m) * np.sum(dz) # b : 1 x 1\n",
    "        \n",
    "        self.w -= lr * dw\n",
    "        self.b -= lr * db\n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            if i % 10000 == 0:\n",
    "                prev_w = self.w\n",
    "                prev_b = self.b\n",
    "                prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "\n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if i % 10000 == 9999:\n",
    "                current_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "                if prev_cost < current_cost: \n",
    "                    self.w = prev_w\n",
    "                    self.b = prev_b\n",
    "                    current_cost = prev_cost\n",
    "                    break\n",
    "            \n",
    "        print(\"Iteration:\", i ,\"Cost:\", current_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / test_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHiddenLayer:\n",
    "    def __init__(self, hidden):\n",
    "        '''\n",
    "            matrix shape (row x column)\n",
    "                x : 2 x ?\n",
    "                \n",
    "                w1 : [1,3,5,7,9] x 2\n",
    "                b1 : [1,3,5,7,9] x 1\n",
    "                a1 : [1,3,5,7,9] x ?\n",
    "                \n",
    "                w2 : 1 x [1,3,5,7,9]\n",
    "                b2 : 1 x 1\n",
    "                a2(y_hat) : 1 x ?\n",
    "        '''\n",
    "        # layer 1 (hidden layer)\n",
    "        self.w1 = np.random.rand(hidden, 2) # weight initialization\n",
    "        self.b1 = np.zeros((hidden, 1)) # bias initialization\n",
    "        # final layer (goes to y)\n",
    "        self.w2 = np.random.rand(1, hidden)\n",
    "        self.b2 = 0\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        \n",
    "        z1 = self.w1 @ x + self.b1 # z1 : [1,3,5,7,9] x ?\n",
    "        a1 = self.sigmoid(z1) # a1 : [1,3,5,7,9] x ?\n",
    "        z2 = self.w2 @ a1 + self.b2 # z2 : 1 x ?\n",
    "        a2 = self.sigmoid(z2) # a2 : 1 x ?\n",
    "        \n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(a2)\n",
    "        else:\n",
    "            return a2\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = y.shape[1]\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        \n",
    "        z1 = self.w1 @ x + self.b1 # z1 : [1,3,5,7,9] x ?\n",
    "        a1 = self.sigmoid(z1) # a1 : [1,3,5,7,9] x ?\n",
    "        z2 = self.w2 @ a1 + self.b2 # z2 : 1 x ?\n",
    "        a2 = self.sigmoid(z2) # a2 : 1 x ?\n",
    "        \n",
    "        m = y.shape[1]\n",
    "        \n",
    "        dz2 = a2 - y # dz2 : 1 x ?\n",
    "        dw2 = (1 / m) * (dz2 @ a1.T) # dw2 : 1 x [1,3,5,7,9]\n",
    "        db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True) # db2 : 1 x ? -> 1 x 1\n",
    "        dz1 = (self.w2.T @ dz2) * (self.sigmoid_derivative(z1)) # dz1 : [1,3,5,7,9] x ? # element wise matrix multiplication\n",
    "        dw1 = (1 / m) * (dz1 @ x.T) # [1,3,5,7,9] x 2\n",
    "        db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True) # db1 : [1,3,5,7,9] x ? -> [1,3,5,7,9] x 1\n",
    "        \n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        \n",
    "        \n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            if i % 10000 == 0:\n",
    "                prev_w1 = self.w1\n",
    "                prev_b1 = self.b1\n",
    "                prev_w2 = self.w2\n",
    "                prev_b2 = self.b2\n",
    "                prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "            \n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if i % 10000 == 9999:\n",
    "                current_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "                if prev_cost < current_cost: \n",
    "                    self.w1 = prev_w1\n",
    "                    self.b1 = prev_b1\n",
    "                    self.w2 = prev_w2\n",
    "                    self.b2 = prev_b2\n",
    "                    current_cost = prev_cost\n",
    "                    break\n",
    "            \n",
    "        print(\"Iteration:\", i ,\"Cost:\", current_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / test_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoHiddenLayer:\n",
    "    def __init__(self, hidden1, hidden2):\n",
    "        '''\n",
    "            matrix shape (row x column)\n",
    "                x : 2 x ?\n",
    "                \n",
    "                w1 : hidden1 x 2\n",
    "                b1 : hidden1 x 1\n",
    "                a1 : hidden1 x ?\n",
    "                \n",
    "                w2 : hidden2 x hidden1\n",
    "                b2 : hidden2 x 1\n",
    "                a2 : hidden2 x ?\n",
    "                \n",
    "                w3 : 1 x hidden2\n",
    "                b3 : 1 x 1\n",
    "                a3(y_hat) : 1 x ?\n",
    "        '''\n",
    "        # layer 1 (hidden1)\n",
    "        self.w1 = np.random.rand(hidden1, 2) # weight initialization\n",
    "        self.b1 = np.zeros((hidden1, 1)) # bias initialization\n",
    "        # layer 2 (hidden2)\n",
    "        self.w2 = np.random.rand(hidden2, hidden1)\n",
    "        self.b2 = np.zeros((hidden2, 1))\n",
    "        # final layer (goes to y)\n",
    "        self.w3 = np.random.rand(1, hidden2)\n",
    "        self.b3 = 0\n",
    "        \n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "\n",
    "    def predict(self, x, classify=False):\n",
    "        \n",
    "        z1 = self.w1 @ x + self.b1 # z1 : hidden1 x ?\n",
    "        a1 = self.sigmoid(z1) # a1 : hidden1 x ?\n",
    "        z2 = self.w2 @ a1 + self.b2 # z2 : hidden2 x ?\n",
    "        a2 = self.sigmoid(z2) # a2 : hidden2 x ?\n",
    "        z3 = self.w3 @ a2 + self.b3 # z3 : 1 x ?\n",
    "        a3 = self.sigmoid(z3) # a3 : 1 x ?\n",
    "        \n",
    "        if classify == True:\n",
    "            classifier = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "            return classifier(a3)\n",
    "        else:\n",
    "            return a3\n",
    "\n",
    "\n",
    "    def cost(self, y_hat, y):\n",
    "        assert len(y_hat) == len(y)\n",
    "\n",
    "        m = y.shape[1]\n",
    "\n",
    "        epsilon = 0.000000001 # prevent -inf in log operation\n",
    "\n",
    "        cost = (-1 / m) * np.sum(y * np.log(y_hat + epsilon) + (1-y) * np.log(1 - y_hat + epsilon))\n",
    "\n",
    "        return cost\n",
    "\n",
    "    def gradient_descent(self, x, y, lr):\n",
    "        \n",
    "        z1 = self.w1 @ x + self.b1 # z1 : hidden1 x ?\n",
    "        a1 = self.sigmoid(z1) # a1 : hidden1 x ?\n",
    "        z2 = self.w2 @ a1 + self.b2 # z2 : hidden2 x ?\n",
    "        a2 = self.sigmoid(z2) # a2 : hidden2 x ?\n",
    "        z3 = self.w3 @ a2 + self.b3 # z3 : 1 x ?\n",
    "        a3 = self.sigmoid(z3) # a3 : 1 x ?\n",
    "        \n",
    "        m = y.shape[1]\n",
    "        \n",
    "        dz3 = a3 - y\n",
    "        dw3 = (1 / m) * (dz3 @ a2.T)\n",
    "        db3 = (1 / m) * np.sum(dz3, axis=1, keepdims=True)\n",
    "        \n",
    "        dz2 = (self.w3.T @ dz3) * (self.sigmoid_derivative(z2))\n",
    "        dw2 = (1 / m) * (dz2 @ a1.T)\n",
    "        db2 = (1 / m) * np.sum(dz2, axis=1, keepdims=True)\n",
    "        \n",
    "        dz1 = (self.w2.T @ dz2) * (self.sigmoid_derivative(z1))\n",
    "        dw1 = (1 / m) * (dz1 @ x.T)\n",
    "        db1 = (1 / m) * np.sum(dz1, axis=1, keepdims=True)\n",
    "        \n",
    "        self.w3 -= lr * dw3\n",
    "        self.b3 -= lr * db3\n",
    "        self.w2 -= lr * dw2\n",
    "        self.b2 -= lr * db2\n",
    "        self.w1 -= lr * dw1\n",
    "        self.b1 -= lr * db1\n",
    "        \n",
    "\n",
    "    def train(self, train_x, train_y, val_x, val_y, lr, iters):\n",
    "        for i in range(iters):\n",
    "            if i % 10000 == 0:\n",
    "                prev_w1 = self.w1\n",
    "                prev_b1 = self.b1\n",
    "                prev_w2 = self.w2\n",
    "                prev_b2 = self.b2\n",
    "                prev_w3 = self.w3\n",
    "                prev_b3 = self.b3\n",
    "                prev_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "            \n",
    "            self.gradient_descent(train_x, train_y, lr) # update w,b\n",
    "            \n",
    "            # stop training if cost has increased in validation set : prevent overfitting\n",
    "            if i % 10000 == 9999:\n",
    "                current_cost = self.cost(self.predict(val_x, classify=False), val_y)\n",
    "                if prev_cost < current_cost: \n",
    "                    self.w1 = prev_w1\n",
    "                    self.b1 = prev_b1\n",
    "                    self.w2 = prev_w2\n",
    "                    self.b2 = prev_b2\n",
    "                    self.w3 = prev_w3\n",
    "                    self.b3 = prev_b3\n",
    "                    current_cost = prev_cost\n",
    "                    break\n",
    "            \n",
    "        print(\"Iteration:\", i ,\"Cost:\", current_cost)\n",
    "                \n",
    "    def accuracy(self, test_x, test_y):\n",
    "        y_hat = self.predict(test_x, classify=True)\n",
    "        return (np.sum(y_hat == test_y) / test_y.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data\n",
    "\n",
    "noises = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "data = [{\"train\" : None, \"val\": None, \"test\": None} for _ in range(len(noises))]\n",
    "\n",
    "for i in range(len(noises)):\n",
    "    train_x = []\n",
    "    train_y = []\n",
    "    train_file = open(\"two_moon_{}/train.txt\".format(noises[i]), \"r\")\n",
    "    for line in train_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        train_x.append([float(l[0]), float(l[1])])\n",
    "        train_y.append([int(l[2])])\n",
    "    train_file.close()\n",
    "    data[i][\"train\"] = (np.array(train_x).T, np.array(train_y).T)\n",
    "\n",
    "    val_x = []\n",
    "    val_y = []\n",
    "    val_file = open(\"two_moon_{}/val.txt\".format(noises[i]), \"r\")\n",
    "    for line in val_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        val_x.append([float(l[0]), float(l[1])])\n",
    "        val_y.append([int(l[2])])\n",
    "    val_file.close()\n",
    "    data[i][\"val\"] = (np.array(val_x).T, np.array(val_y).T)\n",
    "\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    test_file = open(\"two_moon_{}/test.txt\".format(noises[i]), \"r\")\n",
    "    for line in test_file.readlines():\n",
    "        l = line.rstrip().split()\n",
    "        test_x.append([float(l[0]), float(l[1])])\n",
    "        test_y.append([int(l[2])])\n",
    "    test_file.close()\n",
    "    data[i][\"test\"] = (np.array(test_x).T, np.array(test_y).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise :  0.1\n",
      "################################\n",
      "Logistic Regression\n",
      "Iteration: 39999 Cost: 0.2736342444626414\n",
      "Accuracy: 0.84\n",
      "################################\n",
      "One Hidden Layer : 1 Layers\n",
      "Iteration: 999999 Cost: 0.2785162473832272\n",
      "Accuracy: 0.84\n",
      "One Hidden Layer : 3 Layers\n",
      "Iteration: 999999 Cost: 0.008456047660195986\n",
      "Accuracy: 1.0\n",
      "One Hidden Layer : 5 Layers\n",
      "Iteration: 29999 Cost: 0.2820930675915389\n",
      "Accuracy: 0.84\n",
      "One Hidden Layer : 7 Layers\n",
      "Iteration: 29999 Cost: 0.27977694881654624\n",
      "Accuracy: 0.845\n",
      "One Hidden Layer : 9 Layers\n",
      "Iteration: 29999 Cost: 0.2791618555972905\n",
      "Accuracy: 0.845\n",
      "################################\n",
      "Two Hidden Layers : 1 x 1 Layers\n",
      "Iteration: 999999 Cost: 0.27864589496320924\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 3 Layers\n",
      "Iteration: 999999 Cost: 0.2774905610710093\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 5 Layers\n",
      "Iteration: 999999 Cost: 0.27737202105843706\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 7 Layers\n",
      "Iteration: 999999 Cost: 0.2773306638742863\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 9 Layers\n",
      "Iteration: 999999 Cost: 0.2773926582194482\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 3 x 1 Layers\n",
      "Iteration: 389999 Cost: 0.007687527144276351\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 3 x 3 Layers\n",
      "Iteration: 739999 Cost: 0.006195369020193019\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 3 x 5 Layers\n",
      "Iteration: 399999 Cost: 0.007249734837294326\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 3 x 7 Layers\n",
      "Iteration: 579999 Cost: 0.006034508935185275\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 3 x 9 Layers\n",
      "Iteration: 609999 Cost: 0.0061452422408826935\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 5 x 1 Layers\n",
      "Iteration: 599999 Cost: 0.007858853256858773\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 5 x 3 Layers\n",
      "Iteration: 849999 Cost: 0.007013399824715141\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 5 x 5 Layers\n",
      "Iteration: 459999 Cost: 0.0070092249882295135\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 5 x 7 Layers\n",
      "Iteration: 559999 Cost: 0.007011626477429552\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 5 x 9 Layers\n",
      "Iteration: 759999 Cost: 0.006498618342220246\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 7 x 1 Layers\n",
      "Iteration: 749999 Cost: 0.008150300611973266\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 7 x 3 Layers\n",
      "Iteration: 579999 Cost: 0.007073158243509783\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 7 x 5 Layers\n",
      "Iteration: 589999 Cost: 0.007164855149480902\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 7 x 7 Layers\n",
      "Iteration: 489999 Cost: 0.006780001199459463\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 7 x 9 Layers\n",
      "Iteration: 529999 Cost: 0.007021487060254944\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 9 x 1 Layers\n",
      "Iteration: 719999 Cost: 0.008054787717735193\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 9 x 3 Layers\n",
      "Iteration: 519999 Cost: 0.006749836626835339\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 9 x 5 Layers\n",
      "Iteration: 949999 Cost: 0.007455346208611264\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 9 x 7 Layers\n",
      "Iteration: 529999 Cost: 0.006935139313167764\n",
      "Accuracy: 1.0\n",
      "Two Hidden Layers : 9 x 9 Layers\n",
      "Iteration: 539999 Cost: 0.007151374576696017\n",
      "Accuracy: 1.0\n",
      "################################\n",
      "Noise :  0.3\n",
      "################################\n",
      "Logistic Regression\n",
      "Iteration: 999999 Cost: 0.32265309163018424\n",
      "Accuracy: 0.85\n",
      "################################\n",
      "One Hidden Layer : 1 Layers\n",
      "Iteration: 999999 Cost: 0.32368984242143894\n",
      "Accuracy: 0.84\n",
      "One Hidden Layer : 3 Layers\n",
      "Iteration: 549999 Cost: 0.255630427683161\n",
      "Accuracy: 0.895\n",
      "One Hidden Layer : 5 Layers\n",
      "Iteration: 169999 Cost: 0.3238958964210552\n",
      "Accuracy: 0.84\n",
      "One Hidden Layer : 7 Layers\n",
      "Iteration: 299999 Cost: 0.25503741613344916\n",
      "Accuracy: 0.895\n",
      "One Hidden Layer : 9 Layers\n",
      "Iteration: 219999 Cost: 0.2509681973012561\n",
      "Accuracy: 0.895\n",
      "################################\n",
      "Two Hidden Layers : 1 x 1 Layers\n",
      "Iteration: 999999 Cost: 0.32380849594136196\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 3 Layers\n",
      "Iteration: 999999 Cost: 0.3233623480953885\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 5 Layers\n",
      "Iteration: 999999 Cost: 0.3233396411160996\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 7 Layers\n",
      "Iteration: 999999 Cost: 0.3230023123980278\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 1 x 9 Layers\n",
      "Iteration: 999999 Cost: 0.32280765446991394\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 3 x 1 Layers\n",
      "Iteration: 159999 Cost: 0.25297557507714863\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 3 x 3 Layers\n",
      "Iteration: 339999 Cost: 0.32042815645265876\n",
      "Accuracy: 0.835\n",
      "Two Hidden Layers : 3 x 5 Layers\n",
      "Iteration: 519999 Cost: 0.2488357240248109\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 3 x 7 Layers\n",
      "Iteration: 569999 Cost: 0.2527339929036742\n",
      "Accuracy: 0.9\n",
      "Two Hidden Layers : 3 x 9 Layers\n",
      "Iteration: 479999 Cost: 0.25057267125878796\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 5 x 1 Layers\n",
      "Iteration: 129999 Cost: 0.3306832309454413\n",
      "Accuracy: 0.835\n",
      "Two Hidden Layers : 5 x 3 Layers\n",
      "Iteration: 179999 Cost: 0.25055847091301514\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 5 x 5 Layers\n",
      "Iteration: 359999 Cost: 0.2451629453138737\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 5 x 7 Layers\n",
      "Iteration: 359999 Cost: 0.3205199273618372\n",
      "Accuracy: 0.84\n",
      "Two Hidden Layers : 5 x 9 Layers\n",
      "Iteration: 129999 Cost: 0.24915723361390435\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 7 x 1 Layers\n",
      "Iteration: 139999 Cost: 0.25146601889195686\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 7 x 3 Layers\n",
      "Iteration: 159999 Cost: 0.2504355279855981\n",
      "Accuracy: 0.89\n",
      "Two Hidden Layers : 7 x 5 Layers\n",
      "Iteration: 169999 Cost: 0.2485030002927717\n",
      "Accuracy: 0.89\n",
      "Two Hidden Layers : 7 x 7 Layers\n",
      "Iteration: 479999 Cost: 0.25419060467610854\n",
      "Accuracy: 0.89\n",
      "Two Hidden Layers : 7 x 9 Layers\n",
      "Iteration: 129999 Cost: 0.24963010100581073\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 9 x 1 Layers\n",
      "Iteration: 9999 Cost: 0.6929084677003441\n",
      "Accuracy: 0.49\n",
      "Two Hidden Layers : 9 x 3 Layers\n",
      "Iteration: 109999 Cost: 0.25119232077343645\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 9 x 5 Layers\n",
      "Iteration: 109999 Cost: 0.24879993667695777\n",
      "Accuracy: 0.895\n",
      "Two Hidden Layers : 9 x 7 Layers\n",
      "Iteration: 139999 Cost: 0.2413071170149135\n",
      "Accuracy: 0.89\n",
      "Two Hidden Layers : 9 x 9 Layers\n",
      "Iteration: 119999 Cost: 0.2488132650702949\n",
      "Accuracy: 0.895\n",
      "################################\n",
      "Noise :  0.5\n",
      "################################\n",
      "Logistic Regression\n",
      "Iteration: 999999 Cost: 0.45374657233794125\n",
      "Accuracy: 0.85\n",
      "################################\n",
      "One Hidden Layer : 1 Layers\n",
      "Iteration: 999999 Cost: 0.45860153992142166\n",
      "Accuracy: 0.855\n",
      "One Hidden Layer : 3 Layers\n",
      "Iteration: 209999 Cost: 0.4566296343898052\n",
      "Accuracy: 0.84\n",
      "One Hidden Layer : 5 Layers\n",
      "Iteration: 129999 Cost: 0.46024305324488907\n",
      "Accuracy: 0.865\n",
      "One Hidden Layer : 7 Layers\n",
      "Iteration: 159999 Cost: 0.45958900581269474\n",
      "Accuracy: 0.865\n",
      "One Hidden Layer : 9 Layers\n",
      "Iteration: 139999 Cost: 0.459097324843722\n",
      "Accuracy: 0.865\n",
      "################################\n",
      "Two Hidden Layers : 1 x 1 Layers\n",
      "Iteration: 999999 Cost: 0.4590104989583797\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 1 x 3 Layers\n",
      "Iteration: 999999 Cost: 0.45742916763098534\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 1 x 5 Layers\n",
      "Iteration: 999999 Cost: 0.4575632593116508\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 1 x 7 Layers\n",
      "Iteration: 999999 Cost: 0.4572343065420542\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 1 x 9 Layers\n",
      "Iteration: 999999 Cost: 0.4571960671643109\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 3 x 1 Layers\n",
      "Iteration: 189999 Cost: 0.4647344061679024\n",
      "Accuracy: 0.865\n",
      "Two Hidden Layers : 3 x 3 Layers\n",
      "Iteration: 129999 Cost: 0.4599701624093349\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 3 x 5 Layers\n",
      "Iteration: 129999 Cost: 0.4606396768587456\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 3 x 7 Layers\n",
      "Iteration: 159999 Cost: 0.46083735263601877\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 3 x 9 Layers\n",
      "Iteration: 219999 Cost: 0.4607257313252056\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 5 x 1 Layers\n",
      "Iteration: 159999 Cost: 0.4606122125936388\n",
      "Accuracy: 0.85\n",
      "Two Hidden Layers : 5 x 3 Layers\n",
      "Iteration: 219999 Cost: 0.46239883575467206\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 5 x 5 Layers\n",
      "Iteration: 219999 Cost: 0.46007563592393375\n",
      "Accuracy: 0.865\n",
      "Two Hidden Layers : 5 x 7 Layers\n",
      "Iteration: 249999 Cost: 0.45979293162464374\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 5 x 9 Layers\n",
      "Iteration: 149999 Cost: 0.45976141399407994\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 7 x 1 Layers\n",
      "Iteration: 159999 Cost: 0.4598813754679932\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 7 x 3 Layers\n",
      "Iteration: 109999 Cost: 0.46033454914611627\n",
      "Accuracy: 0.855\n",
      "Two Hidden Layers : 7 x 5 Layers\n",
      "Iteration: 189999 Cost: 0.4544192280683015\n",
      "Accuracy: 0.87\n",
      "Two Hidden Layers : 7 x 7 Layers\n",
      "Iteration: 129999 Cost: 0.4634128378952763\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 7 x 9 Layers\n",
      "Iteration: 159999 Cost: 0.4610318041999674\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 9 x 1 Layers\n",
      "Iteration: 119999 Cost: 0.4605844092091089\n",
      "Accuracy: 0.86\n",
      "Two Hidden Layers : 9 x 3 Layers\n",
      "Iteration: 179999 Cost: 0.4577421993484454\n",
      "Accuracy: 0.865\n",
      "Two Hidden Layers : 9 x 5 Layers\n",
      "Iteration: 129999 Cost: 0.45877188353108045\n",
      "Accuracy: 0.845\n",
      "Two Hidden Layers : 9 x 7 Layers\n",
      "Iteration: 119999 Cost: 0.45522923832419765\n",
      "Accuracy: 0.85\n",
      "Two Hidden Layers : 9 x 9 Layers\n",
      "Iteration: 129999 Cost: 0.46379870787634503\n",
      "Accuracy: 0.87\n",
      "################################\n",
      "Noise :  0.7\n",
      "################################\n",
      "Logistic Regression\n",
      "Iteration: 19999 Cost: 0.5119139086060015\n",
      "Accuracy: 0.755\n",
      "################################\n",
      "One Hidden Layer : 1 Layers\n",
      "Iteration: 999999 Cost: 0.5096878618605202\n",
      "Accuracy: 0.76\n",
      "One Hidden Layer : 3 Layers\n",
      "Iteration: 19999 Cost: 0.5185738022060776\n",
      "Accuracy: 0.755\n",
      "One Hidden Layer : 5 Layers\n",
      "Iteration: 109999 Cost: 0.5122491656770237\n",
      "Accuracy: 0.765\n",
      "One Hidden Layer : 7 Layers\n",
      "Iteration: 999999 Cost: 0.4937232158424071\n",
      "Accuracy: 0.725\n",
      "One Hidden Layer : 9 Layers\n",
      "Iteration: 659999 Cost: 0.4940328156884195\n",
      "Accuracy: 0.73\n",
      "################################\n",
      "Two Hidden Layers : 1 x 1 Layers\n",
      "Iteration: 999999 Cost: 0.5093514078195385\n",
      "Accuracy: 0.76\n",
      "Two Hidden Layers : 1 x 3 Layers\n",
      "Iteration: 999999 Cost: 0.5093315936264576\n",
      "Accuracy: 0.76\n",
      "Two Hidden Layers : 1 x 5 Layers\n",
      "Iteration: 999999 Cost: 0.5085537921089329\n",
      "Accuracy: 0.765\n",
      "Two Hidden Layers : 1 x 7 Layers\n",
      "Iteration: 999999 Cost: 0.5089800051252095\n",
      "Accuracy: 0.76\n",
      "Two Hidden Layers : 1 x 9 Layers\n",
      "Iteration: 999999 Cost: 0.5088344700838754\n",
      "Accuracy: 0.76\n",
      "Two Hidden Layers : 3 x 1 Layers\n",
      "Iteration: 619999 Cost: 0.4960225604839748\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 3 x 3 Layers\n",
      "Iteration: 529999 Cost: 0.495607685467532\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 3 x 5 Layers\n",
      "Iteration: 149999 Cost: 0.5112717033714157\n",
      "Accuracy: 0.765\n",
      "Two Hidden Layers : 3 x 7 Layers\n",
      "Iteration: 29999 Cost: 0.5200469594912223\n",
      "Accuracy: 0.755\n",
      "Two Hidden Layers : 3 x 9 Layers\n",
      "Iteration: 129999 Cost: 0.5116712795948493\n",
      "Accuracy: 0.765\n",
      "Two Hidden Layers : 5 x 1 Layers\n",
      "Iteration: 449999 Cost: 0.4967409634976008\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 5 x 3 Layers\n",
      "Iteration: 29999 Cost: 0.5213382020196261\n",
      "Accuracy: 0.755\n",
      "Two Hidden Layers : 5 x 5 Layers\n",
      "Iteration: 509999 Cost: 0.4939621489776178\n",
      "Accuracy: 0.725\n",
      "Two Hidden Layers : 5 x 7 Layers\n",
      "Iteration: 29999 Cost: 0.5194273000189691\n",
      "Accuracy: 0.755\n",
      "Two Hidden Layers : 5 x 9 Layers\n",
      "Iteration: 389999 Cost: 0.5097412857396132\n",
      "Accuracy: 0.76\n",
      "Two Hidden Layers : 7 x 1 Layers\n",
      "Iteration: 349999 Cost: 0.49625330606366314\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 7 x 3 Layers\n",
      "Iteration: 259999 Cost: 0.4971686362412606\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 7 x 5 Layers\n",
      "Iteration: 389999 Cost: 0.49517643282502194\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 7 x 7 Layers\n",
      "Iteration: 319999 Cost: 0.4968899716195093\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 7 x 9 Layers\n",
      "Iteration: 489999 Cost: 0.4943408062361562\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 9 x 1 Layers\n",
      "Iteration: 9999 Cost: 0.6928964363049582\n",
      "Accuracy: 0.535\n",
      "Two Hidden Layers : 9 x 3 Layers\n",
      "Iteration: 199999 Cost: 0.5002212666582746\n",
      "Accuracy: 0.715\n",
      "Two Hidden Layers : 9 x 5 Layers\n",
      "Iteration: 29999 Cost: 0.5180162814085949\n",
      "Accuracy: 0.755\n",
      "Two Hidden Layers : 9 x 7 Layers\n",
      "Iteration: 349999 Cost: 0.49464403195446943\n",
      "Accuracy: 0.735\n",
      "Two Hidden Layers : 9 x 9 Layers\n",
      "Iteration: 29999 Cost: 0.5156084937719093\n",
      "Accuracy: 0.755\n",
      "################################\n",
      "Noise :  0.9\n",
      "################################\n",
      "Logistic Regression\n",
      "Iteration: 19999 Cost: 0.5687201078318489\n",
      "Accuracy: 0.785\n",
      "################################\n",
      "One Hidden Layer : 1 Layers\n",
      "Iteration: 999999 Cost: 0.5617539101978758\n",
      "Accuracy: 0.78\n",
      "One Hidden Layer : 3 Layers\n",
      "Iteration: 589999 Cost: 0.556351988589584\n",
      "Accuracy: 0.795\n",
      "One Hidden Layer : 5 Layers\n",
      "Iteration: 539999 Cost: 0.5564220670183085\n",
      "Accuracy: 0.785\n",
      "One Hidden Layer : 7 Layers\n",
      "Iteration: 439999 Cost: 0.5556124009945327\n",
      "Accuracy: 0.78\n",
      "One Hidden Layer : 9 Layers\n",
      "Iteration: 579999 Cost: 0.5557770446871566\n",
      "Accuracy: 0.78\n",
      "################################\n",
      "Two Hidden Layers : 1 x 1 Layers\n",
      "Iteration: 999999 Cost: 0.5609752085080176\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 1 x 3 Layers\n",
      "Iteration: 999999 Cost: 0.5611932626973049\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 1 x 5 Layers\n",
      "Iteration: 999999 Cost: 0.5609176313024645\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 1 x 7 Layers\n",
      "Iteration: 999999 Cost: 0.5609944056950645\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 1 x 9 Layers\n",
      "Iteration: 999999 Cost: 0.5608463722428512\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 3 x 1 Layers\n",
      "Iteration: 579999 Cost: 0.5569813643031275\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 3 x 3 Layers\n",
      "Iteration: 409999 Cost: 0.5590099103822677\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 3 x 5 Layers\n",
      "Iteration: 459999 Cost: 0.5563557010905492\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 3 x 7 Layers\n",
      "Iteration: 609999 Cost: 0.5572651282103896\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 3 x 9 Layers\n",
      "Iteration: 449999 Cost: 0.5569313437440557\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 5 x 1 Layers\n",
      "Iteration: 449999 Cost: 0.5552543493831671\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 5 x 3 Layers\n",
      "Iteration: 639999 Cost: 0.5564130253284096\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 5 x 5 Layers\n",
      "Iteration: 479999 Cost: 0.5569824782958251\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 5 x 7 Layers\n",
      "Iteration: 499999 Cost: 0.5566683254005298\n",
      "Accuracy: 0.775\n",
      "Two Hidden Layers : 5 x 9 Layers\n",
      "Iteration: 459999 Cost: 0.5550185528150163\n",
      "Accuracy: 0.785\n",
      "Two Hidden Layers : 7 x 1 Layers\n",
      "Iteration: 19999 Cost: 0.6928821915431053\n",
      "Accuracy: 0.465\n",
      "Two Hidden Layers : 7 x 3 Layers\n",
      "Iteration: 499999 Cost: 0.5555493238561932\n",
      "Accuracy: 0.785\n",
      "Two Hidden Layers : 7 x 5 Layers\n",
      "Iteration: 389999 Cost: 0.5562286710751174\n",
      "Accuracy: 0.785\n",
      "Two Hidden Layers : 7 x 7 Layers\n",
      "Iteration: 29999 Cost: 0.5661615812809098\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 7 x 9 Layers\n",
      "Iteration: 769999 Cost: 0.5553652300460279\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 9 x 1 Layers\n",
      "Iteration: 19999 Cost: 0.6929425155903074\n",
      "Accuracy: 0.455\n",
      "Two Hidden Layers : 9 x 3 Layers\n",
      "Iteration: 499999 Cost: 0.555430627410353\n",
      "Accuracy: 0.785\n",
      "Two Hidden Layers : 9 x 5 Layers\n",
      "Iteration: 29999 Cost: 0.5667415454626823\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 9 x 7 Layers\n",
      "Iteration: 449999 Cost: 0.5557085192850957\n",
      "Accuracy: 0.78\n",
      "Two Hidden Layers : 9 x 9 Layers\n",
      "Iteration: 449999 Cost: 0.5556931887739888\n",
      "Accuracy: 0.795\n",
      "################################\n"
     ]
    }
   ],
   "source": [
    "# start training!\n",
    "\n",
    "for i in range(len(noises)):\n",
    "    train_x, train_y = data[i][\"train\"]\n",
    "    val_x, val_y = data[i][\"val\"]\n",
    "    test_x, test_y = data[i][\"test\"]\n",
    "    \n",
    "    print(\"Noise : \", noises[i])\n",
    "    print(\"################################\")\n",
    "    \n",
    "    print(\"Logistic Regression\")\n",
    "    prob_1 = LogisticRegression()\n",
    "    prob_1.train(train_x, train_y, val_x, val_y, lr=0.01, iters=1000000)\n",
    "    print(\"Accuracy:\", prob_1.accuracy(test_x, test_y))\n",
    "    print(\"################################\")\n",
    "    \n",
    "    for n in range(1,10,2):    \n",
    "        print(\"One Hidden Layer : {} Layers\".format(n))\n",
    "        prob_2 = OneHiddenLayer(hidden=n)\n",
    "        prob_2.train(train_x, train_y, val_x, val_y, lr=0.01, iters=1000000)\n",
    "        print(\"Accuracy:\", prob_2.accuracy(test_x, test_y))\n",
    "    print(\"################################\")\n",
    "    \n",
    "    for n1 in range(1,10,2):\n",
    "        for n2 in range(1,10,2):\n",
    "            print(\"Two Hidden Layers : {} x {} Layers\".format(n1, n2))\n",
    "            prob_3 = TwoHiddenLayer(hidden1=n1, hidden2=n2)\n",
    "            prob_3.train(train_x, train_y, val_x, val_y, lr=0.01, iters=1000000)\n",
    "            print(\"Accuracy:\", prob_3.accuracy(test_x, test_y))\n",
    "    print(\"################################\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
